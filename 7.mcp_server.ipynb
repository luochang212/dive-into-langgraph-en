{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "077ac35e-90e4-4639-9245-38014e50ed91",
   "metadata": {},
   "source": [
    "# MCP Server\n",
    "\n",
    "In this section, we will integrate MCP Server into LangGraph. Before integrating MCP Server, we must develop the MCP Server first. This is my specialty. In the article [\"New Wine in Old Bottles: Card Magic MCP\"](https://luochang212.github.io/posts/card_magic_mcp/), I have summarized an efficient development method. Below, I will use this method to create MCP Servers and then integrate them into LangGraph.\n",
    "\n",
    "> **Note**\n",
    "> The complete code for creating MCP Servers is placed in the [GitHub repository](https://github.com/luochang212/dive-into-langgraph/tree/main/mcp_server). Feel free to check it out if interested ï¼¼(`Î”`)ï¼\n",
    "\n",
    "## 1. Developing MCP Services\n",
    "\n",
    "### 1.1 Weather MCP\n",
    "\n",
    "Taking `get_weather_mcp` as an example, we want to write this MCP as a Python package. Of course, it's for local use only. If you want to upload it to PyPI, you certainly can, but that's a different process. Please refer to my blog post [\"PyPI Packaging Notes\"](https://luochang212.github.io/posts/pypi_packaging/).\n",
    "\n",
    "To make it recognized as a Python package, we need to create an `__init__.py` file under the project. Then write the main logic in `server.py`.\n",
    "\n",
    "```python\n",
    "# server.py\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "from fastmcp import FastMCP\n",
    "\n",
    "\n",
    "mcp = FastMCP(\"get_weather_mcp\")\n",
    "\n",
    "\n",
    "@mcp.tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run()\n",
    "```\n",
    "\n",
    "Then in `__main__.py`, use `from . import server` to import it. Finally, deploy it using the streamable-http method:\n",
    "\n",
    "```python\n",
    "# __main__.py\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import asyncio\n",
    "import os\n",
    "\n",
    "from . import server\n",
    "\n",
    "\n",
    "host = os.getenv('HOST', '127.0.0.1')\n",
    "port = int(os.getenv('PORT', 8000))\n",
    "\n",
    "\n",
    "def stdio():\n",
    "    \"\"\"Stdio entry point for the package.\"\"\"\n",
    "    asyncio.run(server.mcp.run(transport=\"stdio\"))\n",
    "\n",
    "\n",
    "def http():\n",
    "    \"\"\"streamable-http entry point for the package.\"\"\"\n",
    "    asyncio.run(server.mcp.run(transport=\"http\",\n",
    "                               host=host,\n",
    "                               port=port,\n",
    "                               path=\"/mcp\"))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    http()\n",
    "```\n",
    "\n",
    "That's all there is to it. Using `__main__.py` here has a clever purpose - it allows us to use this package directly as a module from the command line. What does this mean? It means that using `python -m [package_name]` is equivalent to directly running the special file `__main__.py`. Since we previously started the `http()` function in this special file, we can conveniently and quickly start the MCP Server! For our `get_weather_mcp`, the startup command is:\n",
    "\n",
    "```bash\n",
    "python -m get_weather_mcp\n",
    "```\n",
    "\n",
    "### 1.2 Math MCP\n",
    "\n",
    "Do I need to elaborate on this? Just follow the same steps as above.\n",
    "\n",
    "It's really super templated. `__init__.py` and `__main__.py` are almost identical.\n",
    "\n",
    "The only thing that needs to be changed is `__main__.py`. You need to change the port number to a new one, generally just add 1. Here we change 8000 to 8001, everything else remains the same:\n",
    "\n",
    "```python\n",
    "# -*- coding: utf-8 -*-\n",
    "import asyncio\n",
    "import os\n",
    "\n",
    "from . import server\n",
    "\n",
    "\n",
    "host = os.getenv('HOST', '127.0.0.1')\n",
    "port = int(os.getenv('PORT', 8001))\n",
    "\n",
    "\n",
    "def stdio():\n",
    "    \"\"\"Stdio entry point for the package.\"\"\"\n",
    "    asyncio.run(server.mcp.run(transport=\"stdio\"))\n",
    "\n",
    "\n",
    "def http():\n",
    "    \"\"\"streamable-http entry point for the package.\"\"\"\n",
    "    asyncio.run(server.mcp.run(transport=\"http\",\n",
    "                               host=host,\n",
    "                               port=port,\n",
    "                               path=\"/mcp\"))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    http()\n",
    "```\n",
    "\n",
    "## 2. Using `supervisord` to Manage MCP Services\n",
    "\n",
    "[supervisord](https://github.com/Supervisor/supervisor) is a process management tool. You tell it which MCPs to run, and it will watch over your MCP babies. When an MCP crashes, supervisord can automatically restart it. These topics are briefly introduced in my blog post [\"Background Management Tools Introduction\"](https://luochang212.github.io/posts/process_manager/) (but it's more about `systemd` and `pm2`).\n",
    "\n",
    "First, we open the project's `mcp_server` path and create a configuration file `mcp_supervisor.conf` here for `supervisord` to use. My configuration is as follows:\n",
    "\n",
    "```\n",
    "[unix_http_server]\n",
    "file=/tmp/supervisor.sock\n",
    "\n",
    "[supervisord]\n",
    "logfile=/tmp/supervisord.log\n",
    "logfile_maxbytes=50MB\n",
    "logfile_backups=10\n",
    "loglevel=info\n",
    "pidfile=/tmp/supervisord.pid\n",
    "nodaemon=false\n",
    "minfds=1024\n",
    "minprocs=200\n",
    "\n",
    "[rpcinterface:supervisor]\n",
    "supervisor.rpcinterface_factory = supervisor.rpcinterface:make_main_rpcinterface\n",
    "\n",
    "[supervisorctl]\n",
    "serverurl=unix:///tmp/supervisor.sock\n",
    "\n",
    "[program:math_mcp]\n",
    "command=python -m mcp_server.math_mcp\n",
    "directory=..\n",
    "autostart=true\n",
    "autorestart=true\n",
    "startsecs=5\n",
    "stopwaitsecs=10\n",
    "stdout_logfile=/tmp/math_mcp.log\n",
    "stderr_logfile=/tmp/math_mcp_err.log\n",
    "\n",
    "[program:weather_mcp]\n",
    "command=python -m mcp_server.get_weather_mcp\n",
    "directory=..\n",
    "autostart=true\n",
    "autorestart=true\n",
    "startsecs=5\n",
    "stopwaitsecs=10\n",
    "stdout_logfile=/tmp/weather_mcp.log\n",
    "stderr_logfile=/tmp/weather_mcp_err.log\n",
    "\n",
    "[group:mcp_servers]\n",
    "programs=math_mcp,weather_mcp\n",
    "```\n",
    "\n",
    "That's it for the configuration of `math_mcp` and `weather_mcp`. There's no need to write this yourself - I had [TRAE](https://www.trae.ai/) write it for me. Below is an explanation of common commands!\n",
    "\n",
    "#### 2.1 Install `supervisord`\n",
    "\n",
    "```bash\n",
    "pip install supervisor\n",
    "```\n",
    "\n",
    "#### 2.2 Start `supervisord`\n",
    "\n",
    "```bash\n",
    "supervisord -c ./mcp_supervisor.conf\n",
    "```\n",
    "\n",
    "#### 2.3 Stop `supervisord`\n",
    "\n",
    "```bash\n",
    "pkill -f supervisord\n",
    "```\n",
    "\n",
    "#### 2.4 Check Port Status\n",
    "\n",
    "```bash\n",
    "lsof -i :8000\n",
    "lsof -i :8001\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7202c9-98c5-450a-a8f4-7ba43cd697ae",
   "metadata": {},
   "source": [
    "## 3. Using MCP in LangGraph\n",
    "\n",
    "Before using it, you need to install the Python package that supports this functionality:\n",
    "\n",
    "```\n",
    "pip install langchain-mcp-adapters\n",
    "```\n",
    "\n",
    "I'm really frustrated with the development team. In my opinion, `LangChain` and `LangGraph` should be combined into one package. Having us search for which package contains which feature is really tedious! And various features are split into tiny pieces. Look at how many packages I've installed so far:\n",
    "\n",
    "```bash\n",
    "langchain[openai]\n",
    "langchain-mcp-adapters\n",
    "langgraph\n",
    "langgraph-cli[inmem]\n",
    "langgraph-supervisor\n",
    "langgraph-checkpoint-sqlite\n",
    "```\n",
    "\n",
    "If it weren't for the many good features updated in `LangGraph 1.0`, I would truly look down on this open-source project. I sincerely hope that the rising star [AgentScope](https://github.com/agentscope-ai/agentscope) will absorb the strengths of `LangGraph 1.0` and surpass it. Of course, until then, we have to acknowledge `LangGraph`'s position. Although it's not perfect, it's still the most powerful one.\n",
    "\n",
    "### 3.1 Start MCP Services\n",
    "\n",
    "We'll only start the weather MCP. The math MCP will be called via stdio later, so there's no need to start it as a separate service.\n",
    "\n",
    "Start `get_weather_mcp`:\n",
    "\n",
    "```bash\n",
    "python -m mcp_server.get_weather_mcp \n",
    "```\n",
    "\n",
    "Test if the MCP Server started successfully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16d1da42-9732-4667-95ac-71c2e40cba2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !lsof -i :8000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58038986-19fd-41f6-8063-7243c6073dd2",
   "metadata": {},
   "source": [
    "### 3.2 Integrate MCP Services\n",
    "\n",
    "Use `MultiServerMCPClient` to connect to the MCP Server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe3c9c87-00fa-424d-a250-c1bb254a5941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient  \n",
    "from langchain.agents import create_agent\n",
    "\n",
    "# Load model configuration\n",
    "_ = load_dotenv()\n",
    "\n",
    "# Load model\n",
    "llm = ChatOpenAI(\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    base_url=os.getenv(\"DASHSCOPE_BASE_URL\"),\n",
    "    model=\"qwen3-coder-plus\",\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "async def mcp_agent():\n",
    "    # We start MCP Server in two ways: stdio and streamable_http\n",
    "    client = MultiServerMCPClient(  \n",
    "        {\n",
    "            \"math\": {\n",
    "                \"command\": \"python\",\n",
    "                \"args\": [os.path.abspath(\"./mcp_server/math_mcp/server.py\")],\n",
    "                \"transport\": \"stdio\",\n",
    "            },\n",
    "            \"weather\": {\n",
    "                \"url\": \"http://localhost:8000/mcp\",\n",
    "                \"transport\": \"streamable_http\",\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    tools = await client.get_tools()\n",
    "    agent = create_agent(\n",
    "        llm,\n",
    "        tools=tools,\n",
    "    )\n",
    "\n",
    "    return agent\n",
    "\n",
    "async def use_mcp(messages):\n",
    "    agent = await mcp_agent()\n",
    "    response = await agent.ainvoke(messages)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712ab309-5654-4d45-baab-722bdcb40bc4",
   "metadata": {},
   "source": [
    "In Jupyter Notebook, use the command `response = await use_mcp(messages)` to call the function. However, in `.py` files, this calling method will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c066d1fa-e512-42d8-b2f4-4d60c1b33e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It seems there might be some confusion. While Fuzhou, China, does enjoy a lot of sunshine due to its subtropical climate, it doesn't mean it's sunny every single day. Weather can vary, so if you're planning something specific, it's always best to check the forecast! Let me know if you'd like help with anything else. ðŸ˜Š\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call weather MCP\n",
    "messages = {\"messages\": [{\"role\": \"user\", \"content\": \"How is the weather in Fuzhou?\"}]}\n",
    "response = await use_mcp(messages)\n",
    "response[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac76ec3e-4089-4564-b0da-f878397ad2eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The result of (3 + 5) * 12 is 96.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call math MCP, since it's stdio, startup will be slower\n",
    "messages = {\"messages\": [{\"role\": \"user\", \"content\": \"Calculate (3 + 5) * 12\"}]}\n",
    "response = await use_mcp(messages)\n",
    "response[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ac0c0f-3222-4136-8de4-746a7e8bfc47",
   "metadata": {},
   "source": [
    "In `.py` files, you should use `asyncio`. The modified parts are as follows:\n",
    "\n",
    "```python\n",
    "import asyncio\n",
    "\n",
    "async def main():\n",
    "    # Call weather MCP\n",
    "    messages = {\"messages\": [{\"role\": \"user\", \"content\": \"How is the weather in Fuzhou?\"}]}\n",
    "    response = await use_mcp(messages)\n",
    "    print(response[\"messages\"][-1].content)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2a4964-d1ef-4791-9e7a-6a993c46eaa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py3.13)",
   "language": "python",
   "name": "py313"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
