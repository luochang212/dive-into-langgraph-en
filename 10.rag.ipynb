{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f50058e-9a91-45dc-bc28-b40b8c309b67",
   "metadata": {},
   "source": [
    "# RAG\n",
    "\n",
    "Large language models have training data cutoff dates, so they don't know about events after that date. Additionally, their parameter limits prevent them from containing all professional knowledge. In other words, LLMs lack both real-time capabilities and specialized expertise.\n",
    "\n",
    "How can we make LLMs both real-time and professional? The easiest method is to let them \"cheat with notes.\" A **knowledge base** is like \"cheat notes\" for an LLM. Before answering a question, it first checks the notes to see if there's any content related to the question. If there is, it retrieves that content and combines it with the LLM's reasoning ability to generate the final answer.\n",
    "\n",
    "This \"checking notes\" action is [**RAG**](https://docs.langchain.com/oss/python/langchain/retrieval) (Retrieval-Augmented Generation).\n",
    "\n",
    "> **Note**\n",
    "> \n",
    "> Using a knowledge base can make LLM responses evidence-based and reduce hallucinations, but the cost is the overhead of building and maintaining the knowledge base. Especially when the knowledge base is large, it's worth considering whether the cost is justified. After all, expanding the knowledge base to improve Agent performance is somewhat like \"using the finite to fight the infinite, using the certain to fight the uncertain.\" Although we often mention knowledge bases when talking about RAG, RAG is a retrieval technique that can retrieve anything. Compared to retrieving manually built knowledge bases, retrieving internet content or historical conversations is also possible and offers better value. Your next retrieval target doesn't necessarily have to be a knowledge base."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d550c5-a33d-40d2-ba40-0a4bafbe9986",
   "metadata": {},
   "source": [
    "## 1. Prompt Template\n",
    "\n",
    "RAG is not complicated. It retrieves content related to the user's question from the knowledge base and injects it as context into a **Prompt Template**.\n",
    "\n",
    "Here is a prompt template:\n",
    "\n",
    "```text\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Based on the context provided above, answer the question.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \n",
    "```\n",
    "\n",
    "When using this template, fill `{context}` with the retrieved text and `{question}` with the user's question. Then pass the filled prompt template to the LLM for inference.\n",
    "\n",
    "RAG mainly does two things: first, **retrieving** text related to the user's question from the knowledge base, and second, **concatenating** the retrieved text with the user's question using a prompt template. Concatenation is easy; the difficulty lies mainly in retrieval. In the next section, I'll introduce how to retrieve text related to the user's question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cbd2f4-6eaf-4b27-aab1-c8b4497264a5",
   "metadata": {},
   "source": [
    "### 2.1 Text Vectorization\n",
    "\n",
    "Embedding is a technique that converts text into vectors. It takes a piece of text as input and outputs a fixed-length vector.\n",
    "\n",
    "```\n",
    "\"I really like you\" --> [0.190012, 0.123555, .... ]\n",
    "```\n",
    "\n",
    "The purpose of converting text to vectors is to map semantically similar words to the same vector space. Therefore, after converting a pair of synonyms into vectors, their vector distance is usually closer than other words. For example, \"football\" and \"basketball\" are closer in vector space, while \"football\" and \"basket\" are farther apart. The essence of Embedding is compression. From an encoding perspective, natural language contains redundant information. Embedding is equivalent to re-encoding natural language, expressing the most semantics with the fewest tokens.\n",
    "\n",
    "Embedding also has advantages in multilingual scenarios. A well-trained Embedding model aligns multilingual content at the semantic level. That is, a vector can maintain the same semantics across multiple languages. This characteristic allows LLMs to be inclusive. Even when multilingual materials are added, they won't cause \"understanding\" confusion due to different literal words.\n",
    "\n",
    "### 2.2 Vector Retrieval Principles\n",
    "\n",
    "Since Embedding models have the characteristic of training semantically similar words into nearby vectors, we can convert both the \"user question\" and \"knowledge base content\" into Embedding vectors. Then we calculate the distance between vectors. The smaller the vector distance, the higher the similarity between the texts. Using this principle, we can return the Top-K documents from the knowledge base with the smallest vector distance to the question vector.\n",
    "\n",
    "Let's verify with a simple experiment whether this calculation method can obtain truly relevant text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5206b7a1-8076-45fd-a2d2-66430171626b",
   "metadata": {},
   "outputs": [],
   "source": "from dotenv import load_dotenv\nfrom langchain_community.embeddings import DashScopeEmbeddings\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Load environment variables\n_ = load_dotenv()"
  },
  {
   "cell_type": "markdown",
   "id": "0793ec76-02c1-420e-b32c-9600f538766c",
   "metadata": {},
   "source": [
    "Below we calculate the similarity between each document in the knowledge base and the question to see if semantically similar content has higher cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810e5b54-c075-4206-8665-46fc35b342b4",
   "metadata": {},
   "outputs": [],
   "source": "# User question\nquery = \"Should I give red envelopes to relatives I'm not close with during New Year?\"\n\n# Knowledge base\ndocs = [\n    \"Don't give money to people you don't interact with\",\n    \"Sea urchin tofu is delicious, will eat again\",\n    \"Medium-rare steak drizzled with undercooked cheese\",\n]\n\n# Initialize embedding generator\nembeddings = DashScopeEmbeddings()\n\n# Generate vectors\nqv = embeddings.embed_query(query)\ndv = embeddings.embed_documents(docs)\n\n# Calculate cosine similarity\nsimilarities = cosine_similarity([qv], dv)[0]\nresults = list(enumerate(similarities))\nby_sim = sorted(results, key=lambda r: r[1], reverse=True)\n\n# Higher cosine similarity -> smaller angle between unit vectors -> vectors are closer\nprint(\"Sorted by cosine similarity:\")\nfor i, s in by_sim:\n    print('-', docs[i], s)"
  },
  {
   "cell_type": "markdown",
   "id": "6fddc2cd-c448-4ccf-a5b2-655ea9dd4a01",
   "metadata": {},
   "source": [
    "## 3. Vector Retrieval Pipeline\n",
    "\n",
    "Although the above code can already calculate the similarity between knowledge base content and user questions, there are some issues in the engineering process:\n",
    "\n",
    "- **Issue 1**: Embedding models have input length limits, and long text itself affects vector representation\n",
    "- **Issue 2**: When the knowledge base is large, it's difficult to quickly retrieve Top-K relevant texts\n",
    "\n",
    "To solve **Issue 1**, we need text chunking: splitting the text in the knowledge base into uniformly sized chunks. Then use the Embedding model to convert these chunks into Embedding vectors. To ensure chunks don't lose semantics due to truncation, adjacent chunks should have some overlap. **Issue 2** is generally solved by introducing vector databases, which have mature [ANN](https://milvus.io/docs/single-vector-search.md) algorithms to help us quickly retrieve nearest neighbor vectors.\n",
    "\n",
    "After engineering, our retrieval process becomes more complex. Here is a typical [vector retrieval pipeline](https://docs.langchain.com/oss/python/langchain/retrieval#retrieval-pipeline):\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[\"Sources (Google Drive, Slack, Notion, etc.)\"] --> B[Document Loaders]\n",
    "    B --> C[Documents]\n",
    "    C --> D[Split into chunks]\n",
    "    D --> E[Turn into embeddings]\n",
    "    E --> G[Vector Store]\n",
    "    F[User Query] --> H[Query embedding]\n",
    "    H --> G\n",
    "    G --> I[Retriever]\n",
    "    I --> J[LLM uses retrieved info]\n",
    "    J --> K[Answer]\n",
    "\n",
    "    classDef rounded fill:#e1f5fe,stroke:#01579b,stroke-width:1px,rx:30px,ry:30px;\n",
    "    classDef normal fill:#e1f5fe,stroke:#01579b,stroke-width:2px;\n",
    "    class A,C,F,K rounded;\n",
    "    class B,D,E,G,H,I,J normal;\n",
    "```\n",
    "\n",
    "*\\* Rounded boxes represent data, square boxes represent components.*\n",
    "\n",
    "Since LangChain uses a modular approach, each component is replaceable. The bold parts below list the components in the diagram, with replaceable variants on the right:\n",
    "\n",
    "- **Document Loader**: `TextLoader`, `PyMuPDFLoader`, `WebBaseLoader`\n",
    "- **Document Splitter**: `RecursiveCharacterTextSplitter`\n",
    "- **Embedding Generation**: `DashScopeEmbeddings`, `HuggingFaceEmbeddings`\n",
    "- **Vector Store**: `Chroma`, `Milvus`, `FAISS`\n",
    "- **Retriever**: `EnsembleRetriever`, `BM25Retriever`\n",
    "- **LLM**: `ChatOpenAI`\n",
    "\n",
    "In the next section, we'll implement a vector retrieval pipeline containing all the above components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca34b88-0e79-4943-924c-0e477d103d24",
   "metadata": {},
   "source": [
    "## 4. RAG Based on Vector Retrieval\n",
    "\n",
    "â˜ï¸ðŸ¤“ Just six steps to implement a RAG based on vector retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a928bb92-4049-42dc-848e-c8a9d0a688c0",
   "metadata": {},
   "outputs": [],
   "source": "import os\n\n# Configure UA\nMY_USER_AGENT = (\n    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 \"\n    \"(KHTML, like Gecko) Version/17.0 Safari/605.1.15\"\n)\nos.environ[\"USER_AGENT\"] = MY_USER_AGENT\n\nimport bs4\n\nfrom dotenv import load_dotenv\nfrom langchain_openai import ChatOpenAI\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_community.embeddings import DashScopeEmbeddings\nfrom langchain_core.vectorstores import InMemoryVectorStore\nfrom langchain.tools import tool\nfrom langchain.agents import create_agent\n\n# Load model configuration\n_ = load_dotenv()\n\n# Load model\nllm = ChatOpenAI(\n    model=\"qwen3-max\",\n    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n    base_url=os.getenv(\"DASHSCOPE_BASE_URL\"),\n)"
  },
  {
   "cell_type": "markdown",
   "id": "9674895f-d954-4eba-87ca-50edf505637f",
   "metadata": {},
   "source": [
    "Use `WebBaseLoader` to load the content of [\"Alibaba Releases New Quick BI: Discussing ChatBI's Underlying Architecture, Interaction Design, and Cloud Computing Ecosystem\" (é˜¿é‡Œå‘å¸ƒæ–°ç‰ˆ Quick BIï¼ŒèŠèŠ ChatBI çš„åº•å±‚æž¶æž„ã€äº¤äº’è®¾è®¡å’Œäº‘è®¡ç®—ç”Ÿæ€)](https://luochang212.github.io/posts/quick_bi_intro/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6ed370-bfbb-47ff-a46a-e9345fbb9161",
   "metadata": {},
   "outputs": [],
   "source": "# Load article content\nbs4_strainer = bs4.SoupStrainer(class_=([\"post\"]))\nloader = WebBaseLoader(\n    web_paths=([\"https://luochang212.github.io/posts/quick_bi_intro/\"]),\n    bs_kwargs={\"parse_only\": bs4_strainer},\n    requests_kwargs={\"headers\": {\"User-Agent\": MY_USER_AGENT}},\n)\ndocs = loader.load()\n\nassert len(docs) == 1\n\nprint(f\"Total characters: {len(docs[0].page_content)}\")\nprint(docs[0].page_content[:248])"
  },
  {
   "cell_type": "markdown",
   "id": "7cd95cf8-6b58-46bc-932e-4c9a908d2271",
   "metadata": {},
   "source": [
    "### 4.2 Split Documents\n",
    "\n",
    "Use `RecursiveCharacterTextSplitter` to split the text into chunks for subsequent Embedding calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6b582f-6c04-4a34-a636-107a591bb3ef",
   "metadata": {},
   "outputs": [],
   "source": "# Text chunking\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,  # chunk size (characters)\n    chunk_overlap=200,  # chunk overlap (characters)\n    add_start_index=True,  # track index in original document\n)\nall_splits = text_splitter.split_documents(docs)\n\nprint(f\"Split blog post into {len(all_splits)} sub-documents.\")"
  },
  {
   "cell_type": "markdown",
   "id": "e5b9fc60-6ede-4599-8030-a67dab5ab4b1",
   "metadata": {},
   "source": [
    "### 4.3 Vector Generation\n",
    "\n",
    "Note that both user questions and the knowledge base must use the same Embedding model to generate vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fcbc8a-ea0c-48b7-91d0-eadd2d2689cc",
   "metadata": {},
   "outputs": [],
   "source": "# Initialize embedding generator\nembeddings = DashScopeEmbeddings()"
  },
  {
   "cell_type": "markdown",
   "id": "a04d465b-971e-436e-93b3-8581a9b5675a",
   "metadata": {},
   "source": [
    "### 4.4 Vector Store\n",
    "\n",
    "Here we only use `InMemoryVectorStore` for demonstration. For production projects, please use vector databases like Chroma or Milvus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7166be05-c88a-4d64-bab2-8f7f78560d95",
   "metadata": {},
   "outputs": [],
   "source": "# Initialize in-memory vector store\nvector_store = InMemoryVectorStore(embedding=embeddings)\n\n# Add documents to vector store\ndocument_ids = vector_store.add_documents(documents=all_splits)\n\nprint(document_ids[:2])"
  },
  {
   "cell_type": "markdown",
   "id": "511b5a27-3407-4f73-982c-7da747a45b81",
   "metadata": {},
   "source": [
    "### 4.5 Create Tool\n",
    "\n",
    "Create a tool that can be called by the Agent. This tool retrieves `k=2` text chunks most similar to the `query` from the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23990e95-832f-47cf-982c-a054c0144d30",
   "metadata": {},
   "outputs": [],
   "source": "# Create context retrieval tool\n@tool(response_format=\"content_and_artifact\")\ndef retrieve_context(query: str):\n    \"\"\"Retrieve information to help answer a query.\"\"\"\n    retrieved_docs = vector_store.similarity_search(query, k=2)\n    serialized = \"\\n\\n\".join(\n        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n        for doc in retrieved_docs\n    )\n    return serialized, retrieved_docs"
  },
  {
   "cell_type": "markdown",
   "id": "2078b490-1307-43c1-8a70-049cb326b1e4",
   "metadata": {},
   "source": [
    "### 4.6 Retrieve Text\n",
    "\n",
    "Use the Agent to call the retrieval tool and retrieve context related to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6c5d7a-8409-4d28-97a2-d13863e91266",
   "metadata": {},
   "outputs": [],
   "source": "# Create ReAct Agent\nagent = create_agent(\n    llm,\n    tools=[retrieve_context],\n    system_prompt=(\n        # If desired, specify custom instructions\n        \"You have access to a tool that retrieves context from a blog post. \"\n        \"Use the tool to help answer user queries.\"\n    )\n)\n\n# Invoke Agent\nresponse = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"What are the current limitations of Agent capabilities?\"}]\n})\n\n# # Get Agent's complete response\n# for message in result[\"messages\"]:\n#     message.pretty_print()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f2ff4e-f1f5-4d78-9a9e-b65eb29747f1",
   "metadata": {},
   "outputs": [],
   "source": "# Get Agent's final response\nresponse['messages'][-1].pretty_print()"
  },
  {
   "cell_type": "markdown",
   "id": "63e45df6-3023-4867-8ddf-cd419c57652e",
   "metadata": {},
   "source": [
    "## 5. Keyword Retrieval\n",
    "\n",
    "[BM25](https://en.wikipedia.org/wiki/Okapi_BM25) is a term frequency-based ranking algorithm that can estimate the relevance between documents and a given query. Given a query $Q$ containing keywords $q_1, ..., q_n$, the BM25 score of document $D$ is:\n",
    "\n",
    "$$\\text{score}(D, Q) = \\sum_{i=1}^{n} \\text{IDF}(q_i) \\cdot \\frac{f(q_i, D) \\cdot (k_1 + 1)}{f(q_i, D) + k_1 \\cdot \\left( 1 - b + b \\cdot \\frac{|D|}{\\text{avgdl}} \\right)}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $f(q_i, D)$: The number of times keyword $q_i$ appears in document $D$\n",
    "- $|D|$: The word count of document $D$\n",
    "- $avgdl$: The average document length in the collection\n",
    "- $k_1$: Tunable parameter for controlling term frequency saturation, typically $k_1 \\in [1.2, 2.0]$\n",
    "- $b$: Tunable parameter for controlling document normalization, typically $b = 0.75$\n",
    "- $\\text{IDF}(q_i)$: The IDF (Inverse Document Frequency) weight of keyword $q_i$, measuring how common a word is; more common words have lower values\n",
    "\n",
    "For Chinese keyword retrieval, you need to install a Python package that supports word segmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0058ed7-35d0-48b9-bd13-a9dcc3d6ba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf6508c-be93-45eb-8569-aabbe771d505",
   "metadata": {},
   "source": [
    "### 5.1 Create Retriever\n",
    "\n",
    "We use LangChain's [BM25Retriever](https://docs.langchain.com/oss/python/integrations/retrievers/bm25) to create a retriever and use jieba as its tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbd50fd-5fc6-4aba-83ff-c6f1fe3a6ef6",
   "metadata": {},
   "outputs": [],
   "source": "import jieba\n\nfrom langchain_community.retrievers import BM25Retriever\nfrom langchain_core.documents import Document"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2ec2c5-d240-4116-a3c5-919948e46f5d",
   "metadata": {},
   "outputs": [],
   "source": "def chinese_tokenize(text: str) -> list[str]:\n    \"\"\"Chinese word segmentation function\"\"\"\n    tokens = jieba.lcut(text)\n    return [token for token in tokens if token.strip()]\n\n# 1. Create Chinese retriever using text\ntext_retriever = BM25Retriever.from_texts(\n    [\n        \"What does it mean\",\n        \"That's bad\",\n        \"This small matter doesn't matter\",\n        \"I forgive you on her behalf\",\n    ],\n    k=2,\n    preprocess_func=chinese_tokenize,\n)\n\n# 2. Create Chinese retriever using documents\ndoc_retriever = BM25Retriever.from_documents(\n    [\n        Document(page_content=\"Stir-fried pork with chili noodles\"),\n        Document(page_content=\"Meat, egg, and green onion chicken\"),\n        Document(page_content=\"Now it's not familiar\"),\n        Document(page_content=\"Iron skewers\"),\n    ],\n    k=2,\n    preprocess_func=chinese_tokenize,\n)"
  },
  {
   "cell_type": "markdown",
   "id": "c71dc63e-ed32-43ad-a6a3-9981675d0703",
   "metadata": {},
   "source": [
    "### 5.2 Use Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0832e75a-0cda-40a0-9381-2f64b682d4a2",
   "metadata": {},
   "outputs": [],
   "source": "# Retrieve text\ntext_retriever.invoke(\"A small matter\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36fde06-2f17-4036-b743-8d1ad3813767",
   "metadata": {},
   "outputs": [],
   "source": "# Retrieve documents\ndoc_retriever.invoke(\"Noodles\")"
  },
  {
   "cell_type": "markdown",
   "id": "acdd8211-cb80-4ba6-a6d7-39b01f1ae7e8",
   "metadata": {},
   "source": [
    "### 6.1 RRF Score\n",
    "\n",
    "**RRF** (Reciprocal Rank Fusion) is a classic reranking solution. You can use RRF to integrate scores from multiple retrievers to calculate the final ranking of text chunks.\n",
    "\n",
    "The RRF score of a text chunk can be calculated by the following formula:\n",
    "\n",
    "$$\\text{RRF} = \\sum_{i} \\frac{w_i}{k + r_i}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $w_i$: Weight of the $i$-th retriever, default value is $1.0$\n",
    "- $k$: Smoothing parameter, default value is $60$\n",
    "- $r_i$: Ranking of the document in the $i$-th retriever\n",
    "\n",
    "Hybrid retrieval based on RRF scores can be implemented through vector databases. For details, see the documentation, which won't be elaborated here:\n",
    "\n",
    "- [Milvus](https://milvus.io/docs/multi-vector-search.md)\n",
    "- [Chroma](https://docs.trychroma.com/cloud/search-api/hybrid-search)\n",
    "\n",
    "### 6.2 Agentic Hybrid Search\n",
    "\n",
    "According to first principles, if using a large model can achieve better reranking results, why calculate RRF scores? Below we write some experimental code to verify the effect of using a large model for reranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730c3a2c-76a4-4ab3-bb85-d35efceef841",
   "metadata": {},
   "outputs": [],
   "source": "import random\nfrom typing import List\nfrom pydantic import BaseModel, Field\n\n# This is the user query\nquery = \"Sea otter black history collection\"\n\n# These are text chunks retrieved by vector retrieval\ndense_texts = [\n    \"Some marine creatures litter\",\n    \"Sea otters are so cute\",\n    \"Sea otters smell bad\",\n]\n\n# These are text chunks retrieved by keyword retrieval\nsparse_texts = [\n    \"Sea otters smell bad\",\n    \"Snowy owl black history\",\n]\n\n# Define Agent output format\nclass ReRankOutput(BaseModel):\n    indices: List[int] = Field(description=\"List of indices of recalled text fragments after reranking\")\n\n# Return at most limit text chunks\ndef get_relevant_texts(query: str,\n                       dense_texts: list,\n                       sparse_texts: list,\n                       limit: int = 3):\n\n    # Create context\n    texts = dense_texts + sparse_texts\n\n    # Remove duplicates\n    texts = list(set(texts))\n\n    # Shuffle to eliminate position bias\n    random.shuffle(texts)\n\n    # Explicitly add index id before text chunks\n    texts_with_index = [f\"{i} - {text}\" for i, text in enumerate(texts)]\n\n    context = '\\n\\n'.join(texts_with_index)\n    prompt = \"\\n\".join([\n        f\"{context}\",\n        \"---\",\n        \"Above are multiple text fragments recalled by RAG. Each fragment is in the format [index] - [content].\",\n        f\"Please return at most {limit} indices of text fragments related to the user question (if relevant content is insufficient, fewer than {limit} is allowed).\",\n        \"\\nNotes:\",\n        \"1. Text fragments with higher relevance should be ranked first\",\n        \"2. Returned text fragments must help answer the user question!\",\n        f\"\\nUser question: {query}\",\n        \"List of text fragment indices:\",\n    ])\n\n    # Create Agent with structured output\n    agent = create_agent(\n        model=llm,\n        system_prompt=\"You are a retrieval text relevance reranking assistant\",\n        response_format=ReRankOutput,\n    )\n\n    # Invoke Agent\n    result = agent.invoke(\n        {\"messages\": [{\"role\": \"user\", \"content\": prompt}]},\n    )\n\n    indices = result['structured_response'].indices\n    return [texts[i] for i in indices]"
  },
  {
   "cell_type": "markdown",
   "id": "40dd55e7-1781-410d-9c75-7ed1ccc23476",
   "metadata": {},
   "source": [
    "Call the retrieval text relevance reranking assistant to get the reranked relevance text list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07e8e04-1074-4023-b11b-4691b4fa7405",
   "metadata": {},
   "outputs": [],
   "source": "res = get_relevant_texts(\n    query,\n    dense_texts,\n    sparse_texts,\n)\n\nres"
  },
  {
   "cell_type": "markdown",
   "id": "af1dfc87-f27a-41a1-87e0-3ca6f0a1839e",
   "metadata": {},
   "source": [
    "## 7. RAG Architectures\n",
    "\n",
    "### 7.1 2-Step RAG\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[User Question] --> B[Retrieve Relevant Documents];\n",
    "    B --> C[Generate Answer];\n",
    "    C --> D[Return to User];\n",
    "\n",
    "    style A fill:#38761D,color:#FFFFFF\n",
    "    style B fill:#3C87F0,color:#FFFFFF\n",
    "    style C fill:#3C87F0,color:#FFFFFF\n",
    "    style D fill:#38761D,color:#FFFFFF\n",
    "```\n",
    "\n",
    "### 7.2 Agentic RAG\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[User Question] --> B[\"Agent (LLM)\"];\n",
    "    B --> C{Need External Info?};\n",
    "    \n",
    "    C -- Yes --> D[\"Use tool(s) to search\"];\n",
    "    D --> E{Enough Info to Answer?};\n",
    "\n",
    "    \n",
    "    E -- Yes --> F;\n",
    "    E -- No --> B;\n",
    "    C -- No --> F[Generate Final Answer];\n",
    "    F --> G[Return to User];\n",
    "\n",
    "    classDef start_end fill:#38761D,stroke:#388E3C,color:#fff;\n",
    "    classDef step fill:#3C87F0,stroke:#1976D2,color:#fff;\n",
    "    classDef decision fill:#FF9800,stroke:#F57C00,color:#fff;\n",
    "\n",
    "    class A,G start_end\n",
    "    class B,D,F step\n",
    "    class C,E decision\n",
    "```\n",
    "\n",
    "### 7.3 Hybrid RAG\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[User Question] --> B[Query Enhancement]\n",
    "    B --> C[Retrieve Documents]\n",
    "    C --> D{Enough Info?}\n",
    "    D -- \"No\" --> G[Query Optimization]\n",
    "    E --> F{Answer Quality OK?}\n",
    "    F -- \"No\" --> H{Try Different Method?}\n",
    "    H -- \"Yes\" --> G\n",
    "    G --> C\n",
    "    H -- \"No\" --> I[Return Best Answer]\n",
    "    F -- \"Yes\" --> I\n",
    "    D -- \"Yes\" --> E[Generate Answer]\n",
    "    I --> J[Return to User]\n",
    "\n",
    "    classDef start_end fill:#38761D,stroke:#388E3C,color:#fff\n",
    "    classDef step fill:#3C87F0,stroke:#1976D2,color:#fff\n",
    "    classDef decision fill:#FF9800,stroke:#F57C00,color:#fff\n",
    "\n",
    "    class A,J start_end\n",
    "    class B,C,E,G,I step\n",
    "    class D,F,H decision\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7030099-35a4-4b57-a87b-dfc0219bfe38",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "- [All-in-RAG](https://datawhalechina.github.io/all-in-rag/#/chapter4/11_hybrid_search)\n",
    "- [Multi-Vector Hybrid Search](https://milvus.io/docs/multi-vector-search.md)\n",
    "- [Hybrid Search with RRF](https://docs.trychroma.com/cloud/search-api/hybrid-search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03906c3-36c6-44c9-9318-5be1d77b7260",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}