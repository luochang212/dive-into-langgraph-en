{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22142d03-67ac-4dab-ba27-9f22690244d5",
   "metadata": {},
   "source": [
    "# Context Engineering\n",
    "\n",
    "[Context Engineering](https://docs.langchain.com/oss/python/langchain/context-engineering) is crucial for an Agent to produce correct results. When a model doesn't answer well, it's often not due to insufficient capability, but rather because it didn't receive enough contextual information to infer the correct result. It's necessary to enhance the Agent's ability to acquire and manage context through context engineering.\n",
    "\n",
    "**LangGraph divides context into three types:**\n",
    "\n",
    "- Model Context\n",
    "- Tool Context\n",
    "- Life-cycle Context\n",
    "\n",
    "Regardless of the type of Context, its Schema needs to be defined. In this regard, LangGraph provides considerable flexibility - you can use any of `dataclasses`, `pydantic`, or `TypedDict` to create your Context Schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4412b776-1f74-41e8-9444-f1724bcff581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipynbname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a92bc3-bd5e-4ad9-8b4a-2ae1d0f171cc",
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport uuid\nimport sqlite3\n\nfrom typing import Callable\nfrom dotenv import load_dotenv\nfrom dataclasses import dataclass\nfrom langchain_openai import ChatOpenAI\nfrom langchain.tools import tool, ToolRuntime\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import dynamic_prompt, wrap_model_call, ModelRequest, ModelResponse, SummarizationMiddleware\nfrom langgraph.graph import StateGraph, MessagesState, START, END\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.store.memory import InMemoryStore\nfrom langgraph.store.sqlite import SqliteStore\n\n# Load model configuration\n_ = load_dotenv()\n\n# Load model\nllm = ChatOpenAI(\n    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n    base_url=os.getenv(\"DASHSCOPE_BASE_URL\"),\n    model=\"qwen3-coder-plus\",\n    temperature=0.7,\n)"
  },
  {
   "cell_type": "markdown",
   "id": "833f5631-a9f6-40d5-9e57-d652e347c993",
   "metadata": {},
   "source": [
    "## 1. Dynamically Modifying System Prompts\n",
    "\n",
    "Context engineering is closely related to the middleware and memory from previous chapters. The specific implementation of context depends on middleware, while context storage relies on the memory system. Specifically, LangGraph provides a pre-built `@dynamic_prompt` middleware for dynamically modifying system prompts.\n",
    "\n",
    "Since it's dynamic modification, there must be certain conditions to trigger the modification. Besides developing trigger logic, we also need to obtain immediate variables required by the trigger logic from the Agent. These variables are usually stored in the following three storage media:\n",
    "\n",
    "- **Runtime** - All nodes share one Runtime. At the same moment, all nodes get the same Runtime value. Generally used to store information with high timeliness requirements.\n",
    "- **Short-term Memory (State)** - Passed sequentially between nodes, each node receives the State processed by the previous node. Mainly used to store Prompts and AI Messages.\n",
    "- **Long-term Memory (Store)** - Responsible for persistent storage, can save information across Workflows/Agents. Can be used to store user preferences, previously calculated statistics, etc.\n",
    "\n",
    "The following three examples demonstrate how to use context from Runtime, State, and Store to write trigger conditions.\n",
    "\n",
    "### 1.1 Using `State` to Manage Context\n",
    "\n",
    "Use information contained in `State` to manipulate the system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9eb1bf-37b9-4aab-a8f3-0e1cffca6eb3",
   "metadata": {},
   "outputs": [],
   "source": "@dynamic_prompt\ndef state_aware_prompt(request: ModelRequest) -> str:\n    # request.messages is a shortcut for request.state[\"messages\"]\n    message_count = len(request.messages)\n\n    base = \"You are a helpful assistant.\"\n\n    if message_count > 6:\n        base += \"\\nThis is a long conversation - be extra concise.\"\n\n    # Temporarily print base to see the effect\n    print(base)\n\n    return base\n\nagent = create_agent(\n    model=llm,\n    middleware=[state_aware_prompt]\n)\n\nresult = agent.invoke(\n    {\"messages\": [\n        {\"role\": \"user\", \"content\": \"How is the weather in Guangzhou today?\"},\n        {\"role\": \"assistant\", \"content\": \"The weather in Guangzhou is great\"},\n        {\"role\": \"user\", \"content\": \"What should I eat?\"},\n        {\"role\": \"assistant\", \"content\": \"How about trying lemongrass eel casserole?\"},\n        {\"role\": \"user\", \"content\": \"What is lemongrass?\"},\n        {\"role\": \"assistant\", \"content\": \"Lemongrass, also known as lemon grass, is commonly found in Thai Tom Yum soup and Vietnamese grilled meat dishes\"},\n        {\"role\": \"user\", \"content\": \"Aww, what are we waiting for? Let's go eat!\"},\n    ]},\n)\n\nfor message in result['messages']:\n    message.pretty_print()"
  },
  {
   "cell_type": "markdown",
   "id": "ce9c9a5d-90a9-464d-b0dd-bc0b383ce149",
   "metadata": {},
   "source": [
    "Change the `6` in `message_count > 6` to `7` and see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f4c6d6-cea7-448a-ae42-33bcc64b77a3",
   "metadata": {},
   "source": [
    "### 1.2 Using `Store` to Manage Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98294b9-7a56-499b-a190-e0181e2d6a72",
   "metadata": {},
   "outputs": [],
   "source": "@dataclass\nclass Context:\n    user_id: str\n\n@dynamic_prompt\ndef store_aware_prompt(request: ModelRequest) -> str:\n    user_id = request.runtime.context.user_id\n\n    # Read from Store: get user preferences\n    store = request.runtime.store\n    user_prefs = store.get((\"preferences\",), user_id)\n\n    base = \"You are a helpful assistant.\"\n\n    if user_prefs:\n        style = user_prefs.value.get(\"communication_style\", \"balanced\")\n        base += f\"\\nUser prefers {style} responses.\"\n\n    return base\n\nstore = InMemoryStore()\n\nagent = create_agent(\n    model=llm,\n    middleware=[store_aware_prompt],\n    context_schema=Context,\n    store=store,\n)\n\n# Pre-set two preference records\nstore.put((\"preferences\",), \"user_1\", {\"communication_style\": \"Chinese\"})\nstore.put((\"preferences\",), \"user_2\", {\"communication_style\": \"Korean\"})"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509b7bc4-1a3b-4b32-9afe-3db4894c1ff2",
   "metadata": {},
   "outputs": [],
   "source": "# User 1 prefers concise responses\nresult = agent.invoke(\n    {\"messages\": [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant. Please be extra concise.\"},\n        {\"role\": \"user\", \"content\": 'What is a \"hold short line\"?'}\n    ]},\n    context=Context(user_id=\"user_1\"),\n)\n\nfor message in result['messages']:\n    message.pretty_print()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288d2a65-b328-4448-a155-222afc84f3f8",
   "metadata": {},
   "outputs": [],
   "source": "# User 2 prefers detailed responses\nresult = agent.invoke(\n    {\"messages\": [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant. Please be extra concise.\"},\n        {\"role\": \"user\", \"content\": 'What is a \"hold short line\"?'}\n    ]},\n    context=Context(user_id=\"user_2\"),\n)\n\nfor message in result['messages']:\n    message.pretty_print()"
  },
  {
   "cell_type": "markdown",
   "id": "3ea07a9a-289f-4f4c-987c-a76f4818e753",
   "metadata": {},
   "source": [
    "### 1.3 Using `Runtime` to Manage Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8776b008-b2a7-4947-8bed-f51460d37712",
   "metadata": {},
   "outputs": [],
   "source": "@dataclass\nclass Context:\n    user_role: str\n    deployment_env: str\n\n@dynamic_prompt\ndef context_aware_prompt(request: ModelRequest) -> str:\n    # Read from Runtime Context: user role and environment\n    user_role = request.runtime.context.user_role\n    env = request.runtime.context.deployment_env\n\n    base = \"You are a helpful assistant.\"\n\n    if user_role == \"admin\":\n        base += \"\\nYou can use the get_weather tool.\"\n    else:\n        base += \"\\nYou are prohibited from using the get_weather tool.\"\n\n    if env == \"production\":\n        base += \"\\nBe extra careful with any data modifications.\"\n\n    return base\n\n@tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    return f\"It's always sunny in {city}!\"\n\nagent = create_agent(\n    model=llm,\n    tools=[get_weather],\n    middleware=[context_aware_prompt],\n    context_schema=Context,\n    checkpointer=InMemorySaver(),\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1a91c1-f87f-46ce-b6fe-251040867390",
   "metadata": {},
   "outputs": [],
   "source": "# Use two variables from Runtime to dynamically control the System prompt\n# Set user_role to admin to allow using the weather query tool\nconfig = {'configurable': {'thread_id': str(uuid.uuid4())}}\nresult = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"How is the weather in Guangzhou today?\"}]},\n    context=Context(user_role=\"admin\", deployment_env=\"production\"),\n    config=config,\n)\n\nfor message in result['messages']:\n    message.pretty_print()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51ba924-4b11-4268-a498-29104a3916bd",
   "metadata": {},
   "outputs": [],
   "source": "# If user_role is changed to viewer, the weather query tool cannot be used\nconfig = {'configurable': {'thread_id': str(uuid.uuid4())}}\nresult = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"How is the weather in Guangzhou today?\"}]},\n    context=Context(user_role=\"viewer\", deployment_env=\"production\"),\n    config=config,\n)\n\nfor message in result['messages']:\n    message.pretty_print()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae08b79-0e58-4d4a-90c9-22e9624a4a71",
   "metadata": {},
   "outputs": [],
   "source": "result['messages']"
  },
  {
   "cell_type": "markdown",
   "id": "705e6d02-152c-4a13-a5fc-60e27113d596",
   "metadata": {},
   "source": [
    "## 2. Dynamically Modifying Message Lists\n",
    "\n",
    "LangGraph provides a pre-built middleware `@wrap_model_call` for dynamically modifying message lists. The previous section demonstrated how to obtain context from `State`, `Store`, and `Runtime`. This section will not repeat these demonstrations. In the following example, we mainly demonstrate how to use `Runtime` to inject content from local files into the message list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7b4e4a-1dca-4e40-bafa-840373986c09",
   "metadata": {},
   "outputs": [],
   "source": "@dataclass\nclass FileContext:\n    uploaded_files: list[dict]\n\n@wrap_model_call\ndef inject_file_context(\n    request: ModelRequest,\n    handler: Callable[[ModelRequest], ModelResponse]\n) -> ModelResponse:\n    \"\"\"Inject context about files user has uploaded this session.\"\"\"\n    uploaded_files = request.runtime.context.uploaded_files\n\n    try:\n        base_dir = os.path.dirname(os.path.abspath(__file__))\n    except Exception as e:\n        import ipynbname\n        import os\n        notebook_path = ipynbname.path()\n        base_dir = os.path.dirname(notebook_path)\n\n    file_sections = []\n    for file in uploaded_files:\n        name, ftype = \"\", \"\"\n        path = file.get(\"path\")\n        if path:\n            base_filename = os.path.basename(path)\n            stem, ext = os.path.splitext(base_filename)\n            name = stem or base_filename\n            ftype = (ext.lstrip(\".\") if ext else None)\n\n            # Build file description content\n            content_list = [f\"Name: {name}\"]\n            if ftype:\n                content_list.append(f\"Type: {ftype}\")\n\n            # Resolve relative path to absolute path\n            abs_path = path if os.path.isabs(path) else os.path.join(base_dir, path)\n\n            # Read file content\n            content_block = \"\"\n            if abs_path and os.path.exists(abs_path):\n                try:\n                    with open(abs_path, \"r\", encoding=\"utf-8\") as f:\n                        content_block = f.read()\n                except Exception as e:\n                    content_block = f\"[Error reading file '{abs_path}': {e}]\"\n            else:\n                content_block = \"[File path missing or not found]\"\n\n            section = (\n                f\"---\\n\"\n                f\"{chr(10).join(content_list)}\\n\\n\"\n                f\"{content_block}\\n\"\n                f\"---\"\n            )\n            file_sections.append(section)\n\n        file_context = (\n            \"Loaded session files:\\n\"\n            f\"{chr(10).join(file_sections)}\"\n            \"\\nPlease refer to these files when answering questions.\"\n        )\n\n        # Inject file context before recent messages\n        messages = [  \n            *request.messages,\n            {\"role\": \"user\", \"content\": file_context},\n        ]\n        request = request.override(messages=messages)  \n\n    return handler(request)\n\nagent = create_agent(\n    model=llm,\n    middleware=[inject_file_context],\n    context_schema=FileContext,\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9048a7e0-2131-4841-82f2-b1f7fca1c1e5",
   "metadata": {},
   "outputs": [],
   "source": "result = agent.invoke(\n    {\n        \"messages\": [{\n            \"role\": \"user\",\n            \"content\": \"What should be noted about the faceless passengers in Shanghai Metro?\",\n        }],\n    },\n    context=FileContext(uploaded_files=[{\"path\": \"./docs/rule_horror.md\"}]),\n)\n\nfor message in result['messages']:\n    message.pretty_print()"
  },
  {
   "cell_type": "markdown",
   "id": "59498b8d-9b5d-4cb3-bc86-0230e76a8121",
   "metadata": {},
   "source": [
    "## 3. Using Context in Tools\n",
    "\n",
    "Below, we try to use context information stored in `SqliteStore` in tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33e3357-71e3-4ae3-a956-9e6ad0076a81",
   "metadata": {},
   "outputs": [],
   "source": "# Delete SQLite database\nif os.path.exists(\"user-info.db\"):\n    os.remove(\"user-info.db\")\n\n# Create SQLite storage\nconn = sqlite3.connect(\"user-info.db\", check_same_thread=False, isolation_level=None)\nconn.execute(\"PRAGMA journal_mode=WAL;\")\nconn.execute(\"PRAGMA busy_timeout = 30000;\")\n\nstore = SqliteStore(conn)\n\n# Pre-set two user records\nstore.put((\"user_info\",), \"Liu Ruyan\", {\"description\": \"A cold and talented beauty with extraordinary skills, embarking on a journey into the martial world to uncover the mystery of her origins.\", \"birthplace\": \"Wuxing County\"})\nstore.put((\"user_info\",), \"Su Mubai\", {\"description\": \"A proud swordsman with superb swordsmanship, bearing the blood feud of his family, hiding in the marketplace seeking the truth.\", \"birthplace\": \"Hang County\"})"
  },
  {
   "cell_type": "markdown",
   "id": "c0dd8c15-f80a-4885-9477-37d84bbf9495",
   "metadata": {},
   "source": [
    "### 3.1 Basic Example\n",
    "\n",
    "Using `ToolRuntime`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7547ce-963e-48bf-9855-ab32521c034e",
   "metadata": {},
   "outputs": [],
   "source": "@tool\ndef fetch_user_data(\n    user_id: str,\n    runtime: ToolRuntime\n) -> str:\n    \"\"\"\n    Fetch user information from the in-memory store.\n\n    :param user_id: The unique identifier of the user.\n    :param runtime: The tool runtime context injected by the framework.\n    :return: The user's description string if found; an empty string otherwise.\n    \"\"\"\n    store = runtime.store\n    user_info = store.get((\"user_info\",), user_id)\n\n    user_desc = \"\"\n    if user_info:\n        user_desc = user_info.value.get(\"description\", \"\")\n\n    return user_desc\n\nagent = create_agent(\n    model=llm,\n    tools=[fetch_user_data],\n    store=store,\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60289d99-0235-464c-837c-407065b0096d",
   "metadata": {},
   "outputs": [],
   "source": "result = agent.invoke({\n    \"messages\": [{\n        \"role\": \"user\",\n        \"content\": \"Within five minutes, I want all information about Liu Ruyan\"\n    }]\n})\n\nfor message in result['messages']:\n    message.pretty_print()"
  },
  {
   "cell_type": "markdown",
   "id": "d620c05d-aa96-4d3b-9dfd-9774c99e38f7",
   "metadata": {},
   "source": [
    "### 3.2 More Complex Example\n",
    "\n",
    "Using `ToolRuntime[Context]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1950ba66-93e9-43d2-af5b-b048783287bc",
   "metadata": {},
   "outputs": [],
   "source": "@dataclass\nclass Context:\n    key: str\n\n@tool\ndef fetch_user_data(\n    user_id: str,\n    runtime: ToolRuntime[Context]\n) -> str:\n    \"\"\"\n    Fetch user information from the in-memory store.\n\n    :param user_id: The unique identifier of the user.\n    :param runtime: The tool runtime context injected by the framework.\n    :return: The user's description string if found; an empty string otherwise.\n    \"\"\"\n    key = runtime.context.key\n\n    store = runtime.store\n    user_info = store.get((\"user_info\",), user_id)\n\n    user_desc = \"\"\n    if user_info:\n        user_desc = user_info.value.get(key, \"\")\n\n    return f\"{key}: {user_desc}\"\n\nagent = create_agent(\n    model=llm,\n    tools=[fetch_user_data],\n    store=store,\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd427af-d129-401c-a643-eea926af05d5",
   "metadata": {},
   "outputs": [],
   "source": "result = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Within five minutes, I want all information about Liu Ruyan\"}]},\n    context=Context(key=\"birthplace\"),\n)\n\nfor message in result['messages']:\n    message.pretty_print()"
  },
  {
   "cell_type": "markdown",
   "id": "d3a892c0-3a74-47d4-aef2-dd7bf3270ead",
   "metadata": {},
   "source": [
    "## 4. Compressing Context\n",
    "\n",
    "LangChain provides a built-in middleware `SummarizationMiddleware` for compressing context. This middleware maintains a typical **life-cycle context**. Unlike the transient updates of **model context** and **tool context**, life-cycle context is continuously updated: continuously replacing old messages with summaries.\n",
    "\n",
    "Unless the context is excessively long, causing model capability degradation, there is no need to use `SummarizationMiddleware`. Generally, the threshold for triggering summarization can be set quite high. For example:\n",
    "\n",
    "- `max_tokens_before_summary`: 3000\n",
    "- `messages_to_keep`: 20\n",
    "\n",
    "> If you want to learn more about Context Rot, the Chroma team published [*Context Rot: How Increasing Input Tokens Impacts LLM Performance*](https://research.trychroma.com/context-rot) on July 14, 2025, which systematically reveals the phenomenon of model performance degradation caused by long contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75a7d59-9edf-4a00-af8a-3a6d06d6f27e",
   "metadata": {},
   "outputs": [],
   "source": "# Create short-term memory\ncheckpointer = InMemorySaver()\n\n# Create Agent with built-in summarization middleware\n# The trigger value is set very low to make the configuration work in our example\nagent = create_agent(\n    model=llm,\n    middleware=[\n        SummarizationMiddleware(\n            model=llm,\n            trigger=('tokens', 40),  # Trigger summarization at 40 tokens\n            keep=('messages', 1),  # Keep last 1 messages after summary\n        ),\n    ],\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91068064-ef38-4ae3-841b-5bc7be34c624",
   "metadata": {},
   "outputs": [],
   "source": "result = agent.invoke(\n    {\"messages\": [\n        {\"role\": \"user\", \"content\": \"How is the weather in Guangzhou today?\"},\n        {\"role\": \"assistant\", \"content\": \"The weather in Guangzhou is great\"},\n        {\"role\": \"user\", \"content\": \"What should I eat?\"},\n        {\"role\": \"assistant\", \"content\": \"How about trying lemongrass eel casserole?\"},\n        {\"role\": \"user\", \"content\": \"What is lemongrass?\"},\n        {\"role\": \"assistant\", \"content\": \"Lemongrass, also known as lemon grass, is commonly found in Thai Tom Yum soup and Vietnamese grilled meat dishes\"},\n        {\"role\": \"user\", \"content\": \"Aww, what are we waiting for? Let's go eat!\"},\n    ]},\n    checkpointer=checkpointer,\n)\n\nfor message in result['messages']:\n    message.pretty_print()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725262df-d313-4499-ad09-fe12ac9c7955",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}